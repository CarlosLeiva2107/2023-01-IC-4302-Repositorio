[//]: # (Portada)
# Instituto Tecnológico de Costa Rica

# Escuela de Computación

# Bases de Datos II GR 1

# Resumen 3

# Apache Spark

# Estudiante: 
# Carlos Eduardo Leiva Medaglia / 2021032973

# Profesor: 
# Gerardo Nereo Campos Araya

# I Semestre 2023
[//]: # (Dejo esto para que el siguiente texto inicie en una nueva pagina)
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# Apache Spark
En 2009, la Universidad de California, Berkeley, comenzó el proyecto Apache Spark para diseñar un motor unificado para el procesamiento de datos distribuidos. Spark tiene un modelo de programación similar a MapReduce, pero lo extiende con una abstracción de intercambio de datos llamada "Resilient Distributed Datasets". Esta extensión permite que Spark capture una amplia gama de cargas de trabajo de procesamiento que anteriormente necesitaban motores separados. Spark tiene varios beneficios importantes como:  
-	las aplicaciones son más fáciles de desarrollar porque utilizan una API unificada.  
-	Es más eficiente combinar tareas de procesamiento.  
-	Permite nuevas aplicaciones que no eran posibles con los sistemas anteriores.  


Desde el 2010 se ha convertido en el proyecto de código abierto más activo para el procesamiento de grandes datos, utilizado en más de 1,000 organizaciones en todo el mundo. El proyecto sigue mejorando, construyendo una biblioteca con funciones que van desde la importación de datos hasta el aprendizaje automático. Los usuarios consideran esta capacidad poderosa y la mayoría combina múltiples bibliotecas de Spark en sus aplicaciones.  
# Modelo de Programación
La abstracción principal de programación en Spark son los RDD, que son colecciones tolerantes a fallos de objetos particionados en un clúster y que se pueden manipular en paralelo. Los usuarios crean RDDs aplicando transformaciones. Spark expone RDDs a través de una API de programación funcional en Scala, Java, Python y R, donde los usuarios simplemente pueden pasar funciones locales para ejecutar en el clúster. Las transformaciones devuelven un nuevo objeto RDD que representa el resultado de una computación, pero no lo calculan inmediatamente. Los RDDs proporcionan soporte explícito para el intercambio de datos entre cálculos. Por defecto, los RDDs se vuelven a calcular cada vez que se utilizan en una acción.  
# Las bibliotecas de nivel superior 
-	Spark SQL: Uno de los paradigmas de procesamiento de datos más comunes son las consultas relacionales. Spark, implementa esto, utilizando técnicas similares a las bases de datos analíticas. Además de ejecutar consultas SQL, hemos utilizado el motor Spark SQL para proporcionar una abstracción de nivel superior para las transformaciones básicas de datos. En Spark, estas operaciones se mapean al motor Spark SQL y reciben todas sus optimizaciones.   
-	Spark Streaming: permite el procesamiento de datos de transmisión mediante el procesamiento de pequeños lotes de datos que se combinan regularmente con el estado almacenado en RDDs. Entre sus ventajas tiene que la recuperación de fallas es menos costosa debido al uso de linajes, y es posible combinar streaming con consultas por lotes e interactivas.  
-	GraphX: proporciona una interfaz de cómputo de gráficos similar a Pregel y GraphLab, con soporte para particionamiento de vértices.
-	MLlib: la biblioteca de aprendizaje automático de Spark, que implementa más de 50 algoritmos comunes para el entrenamiento de modelos distribuidos.  







