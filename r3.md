[//]: # (Portada)
# Instituto Tecnológico de Costa Rica

# Escuela de Computación

# Bases de Datos II GR 1

# Resumen 3

# Apache Spark

# Estudiante: 
# Carlos Eduardo Leiva Medaglia / 2021032973

# Profesor: 
# Gerardo Nereo Campos Araya

# I Semestre 2023
[//]: # (Dejo esto para que el siguiente texto inicie en una nueva pagina)
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# Apache Spark
En 2009, la Universidad de California, Berkeley, comenzó el proyecto Apache Spark para diseñar un motor unificado para el procesamiento de datos distribuidos. Spark tiene un modelo de programación similar a MapReduce, pero lo extiende con una abstracción de intercambio de datos llamada "Resilient Distributed Datasets". Esta extensión permite que Spark capture una amplia gama de cargas de trabajo de procesamiento que anteriormente necesitaban motores separados. Spark tiene varios beneficios importantes como:  
-	las aplicaciones son más fáciles de desarrollar porque utilizan una API unificada.  
-	Es más eficiente combinar tareas de procesamiento.  
-	Permite nuevas aplicaciones que no eran posibles con los sistemas anteriores.  


Desde el 2010 se ha convertido en el proyecto de código abierto más activo para el procesamiento de grandes datos, utilizado en más de 1,000 organizaciones en todo el mundo. El proyecto sigue mejorando, construyendo una biblioteca con funciones que van desde la importación de datos hasta el aprendizaje automático. Los usuarios consideran esta capacidad poderosa y la mayoría combina múltiples bibliotecas de Spark en sus aplicaciones.  
# Modelo de Programación
La abstracción principal de programación en Spark son los RDD, que son colecciones tolerantes a fallos de objetos particionados en un clúster y que se pueden manipular en paralelo. Los usuarios crean RDDs aplicando transformaciones. Spark expone RDDs a través de una API de programación funcional en Scala, Java, Python y R, donde los usuarios simplemente pueden pasar funciones locales para ejecutar en el clúster. Las transformaciones devuelven un nuevo objeto RDD que representa el resultado de una computación, pero no lo calculan inmediatamente. Los RDDs proporcionan soporte explícito para el intercambio de datos entre cálculos. Por defecto, los RDDs se vuelven a calcular cada vez que se utilizan en una acción.  
# Las bibliotecas de nivel superior 
-	Spark SQL: Uno de los paradigmas de procesamiento de datos más comunes son las consultas relacionales. Spark, implementa esto, utilizando técnicas similares a las bases de datos analíticas. Además de ejecutar consultas SQL, hemos utilizado el motor Spark SQL para proporcionar una abstracción de nivel superior para las transformaciones básicas de datos. En Spark, estas operaciones se mapean al motor Spark SQL y reciben todas sus optimizaciones.   
-	Spark Streaming: permite el procesamiento de datos de transmisión mediante el procesamiento de pequeños lotes de datos que se combinan regularmente con el estado almacenado en RDDs. Entre sus ventajas tiene que la recuperación de fallas es menos costosa debido al uso de linajes, y es posible combinar streaming con consultas por lotes e interactivas.  
-	GraphX: proporciona una interfaz de cómputo de gráficos similar a Pregel y GraphLab, con soporte para particionamiento de vértices.
-	MLlib: la biblioteca de aprendizaje automático de Spark, que implementa más de 50 algoritmos comunes para el entrenamiento de modelos distribuidos.  


# Usos
Más de 1000 empresas utilizan Spark en áreas que van desde servicios web hasta biotecnología y finanzas. En el ámbito académico, se usa para aplicaciones en varios dominios científicos. Las aplicaciones más comunes de Spark son para el procesamiento por lotes en conjuntos de datos grandes, incluyendo cargas de trabajo de Extracción-Transformación-Carga para convertir datos de un formato sin procesar a un formato más estructurado. Ejemplos de esto incluyen personalización de página y recomendación en Yahoo!, gestión de un lago de datos en Goldman Sachs, minería de grafos en Alibaba, etc.  
Las organizaciones utilizan Spark SQL para consultas relacionales, a menudo a través de herramientas de inteligencia empresarial. Los desarrolladores y científicos de datos pueden utilizar Spark de manera más interactiva. Este uso interactivo es crucial para hacer preguntas más avanzadas y para diseñar modelos que eventualmente conduzcan a aplicaciones de producción y es común en todos los despliegues.  
El procesamiento en tiempo real también es un caso de uso popular, tanto en análisis como en toma de decisiones en tiempo real con consultas por lotes e interactivas. Por ejemplo, la empresa de vídeo Conviva utiliza Spark para mantener continuamente un modelo del rendimiento del servidor de distribución de contenido.

# ¿Por que usar el modelo Spark?  
El proyecto Spark introdujo un modelo y motor de programación unificado para aplicaciones de big data, lo que simplifica el procesamiento escalable de datos. La experiencia muestra que este modelo puede admitir eficientemente las cargas de trabajo actuales y brinda beneficios sustanciales a los usuarios.  
La generalidad de RDDs se estudia desde dos perspectivas, desde un punto de vista de expresividad, se argumenta que RDDs pueden emular cualquier cálculo distribuido, y desde un punto de vista de sistemas, RDDs dan a las aplicaciones control sobre los recursos de cuello de botella más comunes en clústeres y por eso, hacen posible expresar las mismas optimizaciones para estos recursos.  
Los RDDs se basan en la capacidad de MapReduce para emular cualquier cálculo distribuido, pero hacen que esta emulación sea significativamente más eficiente.RDDs permiten compartir datos de manera rápida y emular el "intercambio de datos" en la memoria que ocurriría en un sistema compuesto por procesos de larga duración. Spark puede ejecutar pasos similares a MapReduce en clústeres grandes con una latencia de 100ms, lo que es suficiente para implementar muchas cargas de trabajo intensivas en datos.

> Spark introdujo un modelo de programación y motor unificado para aplicaciones con altos volúmenes de datos. Este modelo admite eficientemente las cargas de trabajo actuales y brinda beneficios sustanciales a los usuarios.







